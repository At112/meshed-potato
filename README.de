# expert-potato
Contents
Setup:	
Installation Instructions:	
1.	Develop REST API Application.	
 1.1.	Test the application locally.	
2.	Create a Jenkins Pipeline	
3.	Attach repo with the Jenkins Pipeline	
4.	Place Pipeline file in GitHub	
5.	Created Credentials in the Jenkins for AWS	
6.	Create Terraform Script	
 6.1.	Prepare Infrastructure, build docker image and deploy app in ECS.	
 6.2.	Canary/Blue-Green Deployment.	
7.	Architecture diagram	
8.	Future Work for Production Environment	

Setup:
    I have built a simple restful application, which is responding to http://thebazarpoint.com/version (link is not working as resources are destroyed) endpoint and display its current version (2.0.0).
Installation Instructions:
  1.	Develop REST API Application.

  Created a expert-potato/project/demo.py file for the REST API. I exposed 5001 port for the application and building this app in flask using flask_restfull.

 
  Create a expert-potato/project/Dockerfile in the same directory and add the exposed port, entry point, python  and set requirement.txt file.
 
  I have created a expert-potato/project/requirement.txt file to add the required modules to run the app. 
 
          	
  Run the below command to create the docker image. 
   “docker build --tag flask-docker-demo-app .”
 
  Now docker image is ready for the application to deploy on any container platform. But I have automated the docker image build and push to the ECR using the terraform script described below.  

 1.1.	Test the application locally. 
      I run the application locally “python3 demo.py” then use postman to run the API call.  
 
2.	Create a Jenkins Pipeline
  I have created a Jenkins Pipeline, which is able to pull the deployment code from the GitHub repo and if there are any changes committed to the “main” branch then it will trigger the pipeline.
  It has two stages which will run the init and plan in first stage and second stage will run the apply. 

3.	Attach repo with the Jenkins Pipeline
  I have added GitHub repository in the Jenkins pipeline shown below.
 
4.	Place Pipeline file in GitHub
  I have created a pipeline file to run the terraform in the below location in Git Repo. 
   “expert-potato/pipeline”
 
5.	Created Credentials in the Jenkins for AWS

  I have created Access key and Secret Key in Jenkins credentials and used the secret name in the pipeline file showing above. 

 
6.	Create Terraform Script

 6.1.	Prepare Infrastructure, build docker image and deploy app in ECS.

 I have prepared the infrastructure script and pushed to the GitHub. From where my Jenkins pipeline is pulling the code and running it automatically. 
 expert-potato/
 I have created below files for terraform.
  1-	File iam.tf is creating IAM role for creation of the Fargate ECS resources.
  2-	File main.tf is the main terraform file to create most of the resources.
      a.	ECS Cluster using default subnet and VPC specify in network.tf
      b.	Security group for the LB to allow traffic on port 80/http.
      c.	ALB for the application
      d.	Listener at port 80/http
      e.	Target group for the loadbalancer
      f.	ECR Repository for the docker image placement
      g.	ECR Lifecycle policy
      h.	Task definition to deploy on the ECS
      i.	ECS Service and attached it to the target group
      j.	Cloud Watch target for logging
      k.	Setup Route53 zone and create CNAME for the ALB
 3-	File network.tf is to specify the VPC and subnets of the infrastructure. 
 4-	File provider.tf is to specify the aws provider for terraform.
 5-	File push.sh is used to build docker image and push it to the repo. 
 6-	File app.json.tpl is used to create the container definition.
 7-	File variables.tf is used to set the prefix, port, region, app source path and tags.

 6.2.	Canary/Blue-Green Deployment. 
   I have setup variable for deploying the Blue-Green Deployment or setting up different environment with the same Terraform code. To achieve the canary deployment we need to create another set of container attach it to the new target group which will be part of the existing ALB. Once we will confirm the new release deployment is ready then we can switch the container target group to the already running one and get zero downtime during the deployment of new release version. 


7.	Future Work for Production Environment
  1-	We can create separate pipelines for the Image build and deploying the new release version. 
  2-	We can create additional listener to the ALB and point the new version target group to the listener to achieve more seamless deployment. 
  3-	Alerting and monitoring can be more mature.
